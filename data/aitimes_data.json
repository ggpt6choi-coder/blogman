[
  {
    "date": "2025-10-26T18:20:00+09:00",
    "title": "리퀴드 AI, 엣지 디바이스용 3B 비전 언어 모델 ‘LFM2-VL-3B’ 공개",
    "article": "'액체 신경망'으로 유명한 리퀴드 AI가 휴대폰과 노트북, 단일 GPU 인스턴스부터 웨어러블 및 기타 임베디드 기기에 이르기까지 다양한 기기 환경에서 실행되는 새로운 비전 언어 모델(VLM)을 출시했다.\n리퀴드 AI는 22일(현지시간) 독자 아키텍처 기반의 새로운 VLM ‘LFM2-VL-3B’를 공개했다.\n이 모델은 기존 4억5000만(450M), 16억(1.6B) 매개변수 모델을 확장한 30억(3B) 매개변수 버전으로, 이미지와 텍스트를 입력해 텍스트를 출력하는 작업에 최적화됐다.\nLFM2 아키텍처 특유의 빠른 처리 속도를 유지하면서 정확도를 향상한 것이 특징이다. 모델은 LEAP과 허깅페이스에서 오픈 소스로 제공된다.\nLFM2-VL-3B의 구조는 언어 타워(Language Tower), 비전 타워(Vision Tower), 프로젝터(Projector) 세 부분으로 나뉜다.\n언어 타워는 'LFM2-2.6B'를 기반으로 한 컨볼루션 및 어텐션 하이브리드 구조를 사용한다. 비전 타워는 4억 매개변수 규모의 'SigLIP2 NaFlex'를 적용, 이미지의 종횡비를 그대로 유지하며 왜곡을 막는다. 프로젝터는 2층 MLP와 픽셀 언셔플(pixel unshuffle) 기술로 이미지 토큰을 압축한 뒤 언어 공간과 통합 처리, 사용자는 재학습 없이 비전 토큰 사용량을 조절할 수 있다.\n인코더는 최대 512×512 해상도의 이미지를 처리할 수 있으며, 더 큰 이미지는 겹치지 않는 512×512 패치로 나눠 처리한다. 섬네일 경로를 사용해 패치로 나눌 때도 전체 이미지의 전반적인 정보를 파악할 수 있다. 예를 들어, 256×384 크기의 이미지는 96개의 토큰으로, 1000×3000 이미지는 1020개의 토큰으로 변환된다. 사용자는 최소·최대 이미지 토큰 수와 타일링 옵션을 조절해 모델 추론 속도와 이미지 처리 품질을 최적화할 수 있다.\nLFM2-VL-3B는 단계적 학습 전략을 사용한다. 먼저 텍스트와 이미지의 비율을 조정하는 중간 학습을 진행하고, 이후 이미지 이해 능력을 높이기 위한 감독 미세조정(SFT)을 수행한다. 학습에는 대규모 오픈 데이터셋과 자체 제작한 합성 데이터가 활용된다.\n주요 벤치마크에서 LFM2-VL-3B는 경량 오픈 VLM 중 경쟁력 있는 성능을 기록했다.\n'MM-IFEval' 51.83, '리얼월드QA' 71.37, 'MM벤치' 79.81, 'POPE' 89.01이며, 언어 이해 능력은 LFM2-2.6B과 유사한 'GPQA' 30%, 'MMLU' 63%를 기록했다.\n모델은 영어, 일본어, 프랑스어, 스페인어, 독일어, 이탈리아어, 포르투갈어, 아랍어, 중국어, 한국어 등 다국어 시각 이해도 지원한다.\nLFM2-VL-3B의 아키텍처는 계산량과 메모리를 소형 장치 수준으로 줄여주며, 이미지 토큰을 압축하고 사용량을 조절할 수 있어 예측 가능한 처리 속도를 제공한다. SigLIP2 400M NaFlex 인코더는 이미지의 종횡비를 유지해 세밀한 시각적 인식을 가능하게 하고, 프로젝터는 토큰 수를 줄여 초당 처리량을 높인다.\n또 연구진은 GGUF 빌드를 공개해 로컬 장치에서도 모델을 구동할 수 있도록 지원하며, 로보틱스, 모바일, 산업용 등 현지 처리와 엄격한 데이터 관리가 필요한 환경에서도 활용할 수 있도록 설계됐다.\n박찬 기자 cpark@aitimes.com",
    "link": "https://www.aitimes.com/news/articleView.html?idxno=203450",
    "type": "AITIMES",
    "newTitle": "리퀴드 AI 최신 VLM 공개 휴대폰 노트북 어디서든 사용 가능",
    "newArticle": [
      {
        "title": "리퀴드 AI 신규 VLM 출시",
        "content": "액체 신경망 기술로 잘 알려진 리퀴드 AI가 혁신적인 새로운 비전 언어 모델(VLM)을 공개하며 AI 분야에 또 한 번의 발자취를 남겼습니다. 이번에 출시된 VLM은 휴대폰, 노트북, 단일 GPU 인스턴스부터 웨어러블 기기와 같은 다양한 임베디드 환경에 이르기까지 광범위한 기기에서 실행될 수 있도록 설계되어 사용자 접근성을 대폭 높였습니다. 리퀴드 AI는 지난 22일, 독자적인 아키텍처를 기반으로 개발된 새로운 VLM인 'LFM2-VL-3B'를 선보였습니다. 이 모델은 기존 4억 5천만 개와 16억 개 매개변수 모델의 성능을 확장한 30억 개 매개변수 버전으로, 이미지와 텍스트를 동시에 입력받아 텍스트 형태의 결과물을 생성하는 작업에 최적화되어 있습니다. LFM2 아키텍처 특유의 빠른 처리 속도를 유지하면서도 정확도를 향상시킨 점이 LFM2-VL-3B의 가장 큰 강점입니다. 이 최신 VLM은 LEAP과 허깅페이스를 통해 오픈 소스로 제공되어 전 세계 개발자들이 자유롭게 활용하고 개선할 수 있도록 지원합니다. 리퀴드 AI의 이번 행보는 AI 모델의 범용성과 성능 향상이라는 두 마리 토끼를 잡으려는 노력을 보여줍니다."
      },
      {
        "title": "LFM2-VL-3B 모델 구조 상세",
        "content": "새롭게 공개된 LFM2-VL-3B 모델은 언어 타워, 비전 타워, 그리고 프로젝터의 세 가지 핵심 구성 요소로 이루어져 있습니다. 언어 타워는 'LFM2-2.6B' 모델을 기반으로 컨볼루션 및 어텐션 하이브리드 구조를 채택하여 언어 처리 능력을 극대화했습니다. 비전 타워에는 4억 개 매개변수 규모의 'SigLIP2 NaFlex'가 적용되었는데, 이는 이미지의 원본 종횡비를 그대로 유지하면서 왜곡 없이 정확한 시각 정보를 인식하는 데 탁월한 성능을 발휘합니다. 특히 프로젝터 부분은 2층 MLP와 픽셀 언셔플 기술을 결합하여 이미지 토큰을 효율적으로 압축하고, 이를 언어 공간과 통합하여 처리합니다. 이러한 구조 덕분에 사용자는 모델을 재학습시킬 필요 없이 비전 토큰의 사용량을 자유롭게 조절할 수 있으며, 이는 모델의 유연성과 효율성을 크게 향상시키는 요소로 작용합니다. LFM2-VL-3B는 다양한 기기 환경에서 최적의 성능을 발휘할 수 있도록 정교하게 설계된 모델 아키텍처를 자랑합니다."
      },
      {
        "title": "다양한 이미지 처리 기능",
        "content": "LFM2-VL-3B 모델은 최대 512×512 해상도의 이미지를 직접 처리할 수 있으며, 이보다 큰 이미지는 겹치지 않는 512×512 크기의 패치로 분할하여 처리하는 방식을 사용합니다. 흥미로운 점은 이러한 패치 분할 과정에서도 '섬네일 경로'를 활용하여 전체 이미지의 전반적인 정보를 파악할 수 있다는 것입니다. 예를 들어, 256×384 크기의 이미지는 96개의 토큰으로 변환되고, 1000×3000과 같이 매우 큰 이미지는 1020개의 토큰으로 변환됩니다. 사용자는 최소 및 최대 이미지 토큰 수, 그리고 타일링 옵션을 조절함으로써 모델의 추론 속도와 이미지 처리 품질을 자신들의 필요에 맞게 최적화할 수 있습니다. 이러한 세밀한 제어 기능은 LFM2-VL-3B가 다양한 이미지 데이터와 처리 요구사항에 유연하게 대응할 수 있도록 만드는 핵심 요소입니다. LFM2-VL-3B는 이미지 처리의 정확성과 효율성을 모두 만족시키는 차세대 VLM입니다."
      },
      {
        "title": "강력한 성능과 다국어 지원",
        "content": "LFM2-VL-3B 모델은 독보적인 성능을 자랑하며 여러 주요 벤치마크에서 경량 오픈 VLM 중 가장 경쟁력 있는 결과를 기록했습니다. 'MM-IFEval'에서는 51.83점, '리얼월드QA'에서는 71.37점, 'MM벤치'에서는 79.81점, 그리고 'POPE'에서는 89.01점이라는 우수한 성적을 거두었습니다. 특히 언어 이해 능력 측면에서도 기존 LFM2-2.6B 모델과 유사한 수준인 'GPQA' 30%, 'MMLU' 63%를 달성하며 강력한 언어 처리 능력을 입증했습니다. LFM2-VL-3B의 또 다른 중요한 특징은 다국어 시각 이해를 지원한다는 점입니다. 영어, 일본어, 프랑스어, 스페인어, 독일어, 이탈리아어, 포르투갈어, 아랍어, 중국어, 한국어 등 다양한 언어를 지원하여 전 세계 사용자들이 언어의 장벽 없이 VLM을 활용할 수 있도록 길을 열었습니다. 이는 LFM2-VL-3B가 글로벌 AI 시장에서 중요한 역할을 할 수 있음을 시사합니다."
      },
      {
        "title": "기기 경량화 및 현지 처리 가능",
        "content": "LFM2-VL-3B 모델의 아키텍처는 계산량과 메모리 사용량을 획기적으로 줄여, 스마트폰이나 웨어러블 기기 같은 소형 장치에서도 원활하게 실행될 수 있도록 최적화되었습니다. 특히 이미지 토큰을 압축하고 사용량을 유연하게 조절하는 기능은 예측 가능한 처리 속도를 보장하며, 이는 실시간 응용 프로그램에서 매우 중요한 요소입니다. SigLIP2 400M NaFlex 인코더는 이미지의 종횡비를 유지하면서도 세밀한 시각적 인식을 가능하게 하며, 프로젝터는 토큰 수를 효과적으로 줄여 초당 처리량을 향상시킵니다. 더 나아가, 연구진은 GGUF 빌드를 공개하여 사용자들이 모델을 로컬 장치에서도 직접 구동할 수 있도록 지원합니다. 이는 로보틱스, 모바일 애플리케이션, 산업 자동화 등 현지에서의 즉각적인 데이터 처리와 엄격한 데이터 관리가 필수적인 환경에서 LFM2-VL-3B가 매우 유용하게 활용될 수 있음을 의미합니다. 리퀴드 AI는 LFM2-VL-3B를 통해 AI 기술의 접근성과 활용도를 한 단계 끌어올렸습니다."
      },
      {
        "title": "개인적인 생각",
        "content": "리퀴드 AI의 LFM2-VL-3B 출시 소식은 AI 기술의 발전 방향에 대해 많은 것을 시사합니다. 단순히 성능 향상에만 집중하는 것이 아니라, 다양한 기기 환경에서의 실질적인 활용 가능성을 높이는 데 초점을 맞춘 점이 인상 깊습니다. 특히 휴대폰, 웨어러블 기기와 같은 임베디드 기기에서의 구동 가능성은 AI가 우리의 일상생활 속으로 더욱 깊숙이 파고들 수 있는 잠재력을 보여줍니다. 이미지 처리에서의 종횡비 유지, 토큰 압축 및 사용량 조절 기능 등은 모델의 효율성과 유연성을 극대화하여 개발자들이 특정 응용 프로그램에 맞게 모델을 최적화할 수 있도록 돕습니다. 또한, GGUF 빌드를 통한 로컬 장치에서의 구동 지원은 데이터 프라이버시와 보안이 중요한 산업 현장에서 AI 도입을 가속화할 수 있는 중요한 발판이 될 것입니다. LFM2-VL-3B와 같은 모델들은 앞으로 AI가 더욱 민주화되고 폭넓게 활용될 미래를 예고하며, AI 기술의 접근성과 실용성이 계속해서 향상될 것으로 기대됩니다."
      }
    ],
    "hashTag": [
      "#리퀴드AI",
      "#액체신경망",
      "#비전언어모델",
      "#VLM",
      "#LFM2VL3B",
      "#인공지능",
      "#AI",
      "#머신러닝"
    ]
  }
]