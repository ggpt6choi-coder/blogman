[
  {
    "date": "2025-11-09T19:50:00+09:00",
    "title": "구글, 인간 뇌처럼 부분별로 모델 학습하는 ‘중첩 학습’ 소개",
    "article": "구글이 인공지능(AI)이 새로운 정보를 학습할 때 기존 지식을 잊지 않고 스스로 개선할 수 있는 지속 학습(continual learning)의 기반을 제시했다. 인간 뇌가 생각하는 것처럼 학습을 여러 부분으로 나눈 것이 특징이다.\n구글은 최근 딥러닝의 근본적 한계인 망각 문제를 해결하기 위한 새로운 학습 패러다임 ‘중첩 학습(Nested Learning)’을 '뉴립스(NeurIPS) 2025'를 통해 공개했다.\n이 접근법은 대규모 AI 모델을 하나의 단일 네트워크로 보는 대신, 인간 뇌처럼 여러 부분이 연결된 구조로 이해한다. 이를 통해 전체 네트워크를 한꺼번에 훈련하는 기존 방식에서 벗어난다. 즉, 구글의 중첩 학습은 더 복잡하고 유연한 구조다.\n중첩 학습에서는 모델이 여러 층으로 나뉘어 있고, 각 층이 서로 다른 속도로 업데이트된다. 빠르게 변하는 매개변수는 ‘내부 레벨’에서 작동하고, 천천히 변하는 매개변수는 ‘외부 레벨’을 담당한다.\n이렇게 여러 속도로 학습이 이뤄지면, 모델은 단기 정보부터 장기적인 패턴까지 폭넓게 이해할 수 있다. 이 구조를 통해 만들어진 신경학습 모듈인 ‘연속체 메모리 시스템(CMS)’은 마치 기억을 여러 단계로 쌓아가는 두뇌처럼, 다양한 시간대의 정보를 동시에 학습하도록 돕는다.\n각 레벨은 자신이 관찰한 입력, 그래디언트(gradient), 그리고 상태 변화를 하나의 문맥으로 압축해 저장한다. 이런 구조 덕분에 모델은 짧은 순간의 변화부터 오랜 시간에 걸친 흐름까지, 다양한 시간 단위의 정보를 함께 기억할 수 있다.\n연구진은 이런 방식이 기존의 역전파(back-propagation)나 선형 어텐션(linear attention), 그리고 아담(Adam), SGD 같은 일반적인 옵티마이저의 작동 원리를 모두 포괄한다고 설명했다.\n옵티마이저를 단순히 수학 계산을 위한 도구가 아니라 연관 기억(associative memory), 즉 정보를 저장하고 연결하는 일종의 기억 장치로 봤다. 예를 들어, 기존의 모멘텀 방식은 과거의 그래디언트 값을 단순히 저장하는 선형적인 메모리 구조로 이해할 수 있다.\n이를 확장해 ‘딥 모멘텀 그래디언트 디센트(Deep Momentum Gradient Descent)’라는 새로운 방식을 제안했다. 이 방법은 모멘텀 상태를 신경망 기반 메모리로 표현해, 한정된 기억 공간 안에서도 그래디언트의 변화를 더 잘 기록하고 이를 학습 과정에 효율적으로 활용할 수 있도록 한다.\n구글은 중첩 학습의 아이디어를 실제 모델에 적용해 ‘HOPE(Self-modifying Sequence Model)’이라는 새로운 시스템을 만들었다.\nHOPE는 기존의 장기 메모리 모델인 ‘타이탄스(Titans)’를 발전시킨 것으로, 가장 큰 특징은 스스로 학습 방법을 바꾸며 발전할 수 있다는 점이다. 또 CMS을 함께 사용해, 어떤 부분은 빠르게 정보를 기억하고 다른 부분은 천천히 장기 패턴을 쌓아가는 방식으로 작동한다.\n이를 통해 HOPE는 짧은 순간의 정보와 오랜 시간의 흐름을 모두 이해하며, 지속적으로 학습하고 적응할 수 있는 모델로 발전했다고 설명했다.\nHOPE는 언어 모델링과 상식 추론 벤치마크에서 트랜스포머++(Transformer++), 렛넷(RetNet), 타이탄스 등 기존 시스템에 비해 향상된 성능을 보였다.\n평가에는 위키(Wiki), 람바다(LAMBADA), PIQA, 헬라스웨그(HellaSwag), ARC 챌린지, BoolQ 등의 데이터셋이 활용됐으며, 모델은 3억4000만~13억 매개변수 규모로 실험됐다.\n연구진은 중첩 학습이 단순히 모델 구조를 조금 바꾸는 것을 넘어서, 딥러닝의 기본 수학 원리 자체를 새롭게 정의하려는 시도라고 강조했다.\n“중첩 학습은 기존의 심층신경망을 정보를 압축하며 학습하는 계층 구조로 다시 보는 방법이며, 이것이 대형언어모델(LLM)에서 나타나는 문맥 내 학습(in-context learning) 현상을 이해하는 중요한 열쇠가 될 수 있다”라고 설명했다.\n한편 모델의 미세조정 사례가 부쩍 늘어나며, 이처럼 기존에 학습한 일부 능력을 잃는 ‘재앙적 망각(catastrophic forgetting)’ 현상을 줄이려는 연구도 이어지고 있다.\n미국 일리노이대학교 어바나-샴페인 캠퍼스 연구팀은 지난달 모델의 일부 계층만 선택적으로 조정하는 접근법을 소개했다. 또, MIT는 지난 9월 강화 학습(RL)이 지도학습 기반 미세조정(SFT)보다 망각에 유리하다는 연구 결과를 발표했다.\n박찬 기자 cpark@aitimes.com",
    "link": "https://www.aitimes.com/news/articleView.html?idxno=203840",
    "type": "AITIMES",
    "newTitle": "구글, 뇌처럼 학습하는 중첩 학습으로 AI 기억력 향상",
    "newArticle": [
      {
        "title": "구글 AI의 혁신",
        "content": "구글이 인공지능 AI 분야에 새로운 지평을 열었습니다. 기존 AI 학습 방식의 근본적인 한계였던 망각 문제를 해결할 수 있는 '중첩 학습'이라는 혁신적인 패러다임을 공개했습니다. 이는 마치 인간의 뇌가 정보를 처리하는 방식과 유사하게, AI 학습 과정을 여러 부분으로 나누어 효율성을 극대화하는 새로운 접근법입니다. 중첩 학습은 대규모 AI 모델을 단일 네트워크로 보는 대신, 인간 뇌처럼 여러 부분이 유기적으로 연결된 구조로 이해함으로써, 기존의 전체 네트워크를 한꺼번에 훈련하는 방식에서 벗어나 더욱 복잡하고 유연한 학습이 가능해집니다. 이러한 중첩 학습의 핵심은 AI 모델이 다양한 속도로 변화하는 정보를 효과적으로 학습하고 기억할 수 있도록 하는 데 있습니다. 이는 AI가 새로운 정보를 학습할 때 기존 지식을 잊지 않고 스스로 계속해서 개선될 수 있는 지속 학습 AI의 기반을 제시합니다."
      },
      {
        "title": "인간 뇌 닮은 학습 구조",
        "content": "구글이 제안한 중첩 학습은 AI 모델을 여러 층으로 나누고, 각 층이 서로 다른 속도로 매개변수를 업데이트하도록 설계되었습니다. 빠르게 변화하는 단기 정보는 '내부 레벨'에서, 천천히 변하는 장기적인 패턴은 '외부 레벨'에서 담당합니다. 이러한 다층적인 학습 속도 조절을 통해 AI 모델은 단기 기억부터 장기 패턴까지 폭넓은 시간대의 정보를 동시에 이해하고 학습할 수 있게 됩니다. 이 구조를 통해 구현된 '연속체 메모리 시스템(CMS)'은 마치 인간의 뇌가 기억을 여러 단계로 쌓아가는 것처럼, 다양한 시간대의 정보를 효과적으로 학습하고 통합하는 데 기여합니다. 각 레벨은 자신이 관찰한 입력, 그래디언트, 그리고 상태 변화를 하나의 문맥으로 압축하여 저장함으로써, 짧은 순간의 변화부터 오랜 시간에 걸친 흐름까지 모든 정보를 함께 기억하는 지속 학습 AI를 실현합니다."
      },
      {
        "title": "새로운 옵티마이저 개념",
        "content": "중첩 학습은 기존의 역전파, 선형 어텐션, 그리고 아담(Adam)이나 SGD 같은 일반적인 옵티마이저의 작동 원리를 모두 포괄하는 새로운 관점을 제시합니다. 구글은 여기서 한 발 더 나아가, 옵티마이저를 단순한 수학 계산 도구가 아닌 '연관 기억' 즉, 정보를 저장하고 연결하는 일종의 기억 장치로 재해석했습니다. 예를 들어, 기존의 모멘텀 방식은 과거 그래디언트 값을 선형적으로 저장하는 메모리 구조로 볼 수 있습니다. 이를 확장하여 '딥 모멘텀 그래디언트 디센트'라는 새로운 방식을 제안했는데, 이는 모멘텀 상태를 신경망 기반 메모리로 표현하여 한정된 기억 공간 안에서도 그래디언트의 변화를 더 효과적으로 기록하고 학습 과정에 활용할 수 있도록 합니다. 이러한 접근은 지속 학습 AI의 성능을 향상시키는 데 중요한 역할을 합니다."
      },
      {
        "title": "HOPE 시스템의 등장",
        "content": "중첩 학습의 아이디어를 실제 AI 모델에 적용한 결과, 구글은 'HOPE(Self-modifying Sequence Model)'라는 새로운 시스템을 선보였습니다. HOPE는 기존의 장기 메모리 모델인 '타이탄스(Titans)'를 발전시킨 형태로, 가장 큰 특징은 스스로 학습 방법을 변경하며 발전할 수 있다는 점입니다. 이는 '연속체 메모리 시스템(CMS)'과의 결합을 통해 더욱 강화됩니다. HOPE는 어떤 부분에서는 빠른 정보 습득을, 다른 부분에서는 장기적인 패턴 학습을 수행하며, 단기 기억과 장기 흐름을 모두 이해하는 진정한 의미의 지속 학습 AI로 발전했습니다. 이는 AI가 끊임없이 변화하는 환경에 적응하고 스스로를 개선해나가는 능력을 갖추게 되었음을 시사합니다."
      },
      {
        "title": "HOPE의 뛰어난 성능",
        "content": "새롭게 개발된 HOPE 시스템은 다양한 언어 모델링 및 상식 추론 벤치마크에서 기존의 최신 시스템들보다 향상된 성능을 보여주었습니다. 위키, 람바다, PIQA, 헬라스웨그, ARC 챌린지, BoolQ 등 여러 데이터셋을 활용한 평가에서 HOPE는 트랜스포머++, RetNet, 타이탄스 등의 성능을 뛰어넘었습니다. 특히 3억 4천만에서 13억 개의 매개변수를 가진 모델 규모로 실험이 진행되었음에도 불구하고 이러한 우수한 결과를 달성했습니다. 이는 중첩 학습이라는 새로운 학습 패러다임이 실제 AI 성능 향상에 크게 기여하며, 지속 학습 AI 기술의 가능성을 보여주는 중요한 사례입니다. HOPE는 앞으로 더욱 발전된 AI 모델 개발의 밑거름이 될 것으로 기대됩니다."
      },
      {
        "title": "딥러닝의 새로운 정의",
        "content": "연구진은 중첩 학습이 단순히 모델 구조를 약간 변경하는 수준을 넘어, 딥러닝의 기본적인 수학 원리 자체를 새롭게 정의하려는 시도라고 강조했습니다. 중첩 학습은 기존의 심층 신경망을 정보를 압축하며 학습하는 계층 구조로 재해석하는 방법론이며, 이는 대규모 언어 모델(LLM)에서 나타나는 '문맥 내 학습(in-context learning)' 현상을 이해하는 데 중요한 열쇠가 될 수 있습니다. 또한, 최근 모델의 미세조정 사례가 늘면서 발생하는 '재앙적 망각' 현상을 줄이려는 연구의 중요성이 더욱 커지고 있습니다. 일리노이대학교의 계층 선택적 조정 접근법이나 MIT의 강화 학습이 미세조정보다 망각에 유리하다는 연구 결과들은 지속 학습 AI의 실용적인 측면을 강화하는 데 기여하고 있습니다."
      },
      {
        "title": "개인적인 생각",
        "content": "구글의 중첩 학습 연구는 AI가 단순한 도구를 넘어 스스로 발전하고 학습하는 진정한 지능으로 나아가는 중요한 발걸음입니다. 인간 뇌의 작동 방식을 모방하여 학습 효율성과 기억력을 극대화하려는 시도는 매우 흥미롭습니다. 특히, AI가 새로운 정보를 학습하면서 기존 지식을 잊어버리는 '망각' 문제를 해결하는 것은 지속 학습 AI 구현의 핵심 과제이며, 중첩 학습이 이 문제에 대한 강력한 해결책을 제시하고 있다는 점에서 큰 기대를 모읍니다. 'HOPE' 시스템의 성공적인 성능 검증은 이러한 이론적 가능성이 실제 AI 성능으로 이어진다는 것을 보여줍니다. 앞으로 중첩 학습이 더욱 발전하여, AI는 물론이고 우리가 일상생활에서 사용하는 다양한 기술들에 더욱 지능적이고 능동적으로 적용될 것으로 예상됩니다. 이는 AI 기술의 발전을 가속화하고, 인간과 AI의 협력을 더욱 강화하는 계기가 될 것입니다. 또한, 재앙적 망각과 같은 문제 해결 연구와 결합된다면, AI는 더욱 안정적이고 신뢰할 수 있는 동반자로 발전할 가능성이 높습니다."
      }
    ],
    "hashTag": [
      "#구글AI",
      "#중첩학습",
      "#지속학습",
      "#인공지능",
      "#딥러닝",
      "#뉴립스2025",
      "#연속체메모리시스템",
      "#HOPE",
      "#AI기술",
      "#망각문제해결"
    ]
  }
]